import os
import subprocess
from typing import Dict, Any, Optional, Callable
from datetime import datetime
from app.schemas_new.agentic_state import AgenticState, StageEvent
from app.utils.logging import get_logger
from app.config import USE_GEMINI_FOR_PATIENT_SUMMARY, GEMINI_PATIENT_SUMMARY_API_KEY, LOCAL_LLM_PATH

logger = get_logger(__name__)

# ---------------------------
# Helper: Run Local LLaMA
# ---------------------------
def run_local_llama(prompt: str, temperature: float = 0.8, threads: int = 8) -> str:
    """Runs local LLaMA to generate referral draft text."""
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
    llama_exe_path = os.path.join(project_root, "models", "llama-b6018-bin-win-cpu-x64", "llama-run.exe")

    if not os.path.exists(llama_exe_path):
        logger.error(f"[REFERRAL_DRAFT_NODE] LLaMA binary not found at {llama_exe_path}")
        return ""

    try:
        result = subprocess.run(
            [
                llama_exe_path,
                LOCAL_LLM_PATH,
                "--temp", str(temperature),
                "-t", str(threads),
                "--prompt", prompt
            ],
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()
    except Exception as e:
        logger.error(f"[REFERRAL_DRAFT_NODE] Local LLaMA failed: {e}")
        return ""

# ---------------------------
# Helper: Save .eml file
# ---------------------------
def save_referral_as_eml(draft_text: str, session_id: str) -> str:
    os.makedirs("tmp_email_drafts", exist_ok=True)
    filename = f"tmp_email_drafts/referral_{session_id}.eml"
    eml_content = f"""From: gpclinic@example.com
To: specialist@example.com
Subject: Referral
Date: {datetime.now().strftime("%a, %d %b %Y %H:%M:%S")}
MIME-Version: 1.0
Content-Type: text/plain; charset="UTF-8"

{draft_text}
"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(eml_content)
    return filename

# ---------------------------
# Referral Draft Node
# ---------------------------
async def generate_referral_draft_node(
    state: AgenticState,
    ws_send: Optional[Callable[[Dict[str, Any]], Any]] = None
) -> Dict[str, Any]:
    """Generates a referral letter if referral is required and saves a .eml file."""
    updates: Dict[str, Any] = {}

    if not getattr(state, "referral_required", False):
        reasoning = "Referral not required, skipping draft generation."
        logger.info(f"[REFERRAL_DRAFT_NODE] {reasoning}")
        updates["reasoning_trail"] = getattr(state, "reasoning_trail", []) + [reasoning]
        return updates

    # ---------------------------
    # Safe data extraction with fallbacks
    # ---------------------------
    patient_name = getattr(state, "patient_info", "<PATIENT>")
    soap_text = getattr(state, "soap_text", "<SOAP_NOTE>")
    predicted_codes = getattr(state, "predicted_service_codes", [])
    diagnoses_str = ", ".join([sc.code for sc in predicted_codes]) if predicted_codes else "<DIAGNOSES>"
    referral_reason = getattr(state, "referral_rule_applied", "<REFERRAL_REASON>")
    suggested_specialist = getattr(state, "suggested_specialist", "<SPECIALIST>")

    # ---------------------------
    # Prepare AI prompt
    # ---------------------------
    prompt = f"""
You are a medical assistant AI.
Generate a concise, professional referral letter for a specialist.
Include:

- Patient info: {patient_name}
- Relevant history and SOAP note: {soap_text}
- Diagnoses / predicted codes: {diagnoses_str}
- Reason for referral: {referral_reason}
- Suggested specialist or department: {suggested_specialist}
- Polite closing and signature

Do not make assumptions beyond the provided information.
"""

    # ---------------------------
    # Generate draft using AI
    # ---------------------------
    referral_draft = ""
    try:
        if USE_GEMINI_FOR_PATIENT_SUMMARY and GEMINI_PATIENT_SUMMARY_API_KEY:
            import google.generativeai as genai
            genai.configure(api_key=GEMINI_PATIENT_SUMMARY_API_KEY)
            gemini_model = genai.GenerativeModel("gemini-1.5-flash")
            response = gemini_model.generate_content(prompt)
            referral_draft = response.text.strip()
        else:
            referral_draft = run_local_llama(prompt)
    except Exception as e:
        logger.error(f"[REFERRAL_DRAFT_NODE] AI generation failed: {e}")

    if not referral_draft:
        referral_draft = "Referral draft could not be generated by AI."
        logger.warning("[REFERRAL_DRAFT_NODE] Using fallback draft.")

    updates["referral_draft_text"] = referral_draft

    # ---------------------------
    # Save .eml file
    # ---------------------------
    session_id = getattr(state, "session_id", "unknown")
    eml_path = save_referral_as_eml(referral_draft, session_id)
    updates["referral_eml_path"] = eml_path

    # ---------------------------
    # Reasoning trail & stage
    # ---------------------------
    reasoning_trail = getattr(state, "reasoning_trail", [])
    reasoning_trail.append(f"Generated referral draft. .eml saved at {eml_path}")
    updates["reasoning_trail"] = reasoning_trail

    stages = getattr(state, "stages", [])
    stages.append(StageEvent(
        code="generate_referral_draft",
        description="Referral draft generated",
        data={
            "referral_draft_preview": referral_draft[:200] + ("..." if len(referral_draft) > 200 else ""),
            "referral_eml_path": eml_path
        }
    ))
    updates["stages"] = stages

    # ---------------------------
    # WebSocket update
    # ---------------------------
    if ws_send:
        try:
            await ws_send({
                "event_type": "node_update",
                "node": "generate_referral_draft",
                "payload": state.model_copy(update=updates).dict()
            })
        except Exception as e:
            logger.error(f"[REFERRAL_DRAFT_NODE] WS update failed: {e}")

    return updates
